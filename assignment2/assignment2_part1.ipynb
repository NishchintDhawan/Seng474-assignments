{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text   \n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...  \\\n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/nikjohn7/Disaster-Tweets-Kaggle/main/data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many rows and columns are in the data set?\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show rows for the United States\n",
    "# df[df['country'] == 'USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show rows for Canada\n",
    "# df[ df[\"country_name\"] == 'Canada' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'own', 'do', \"hadn't\", 'shan', 'weren', 'doesn', 'her', 'if', 'out', 'here', 'storm', 'down', 'm', 'o', 'trapped', 'mustn', 'an', 'ma', 'i', \"that'll\", 'some', \"couldn't\", 're', 'fall', 'themselves', 'such', \"hasn't\", 'on', \"don't\", 'Windstorm', 'they', 'Ablaze', 'theirs', 'wouldn', 'does', 'yours', 'further', 'now', 'over', \"aren't\", 'are', \"mightn't\", 'any', \"isn't\", 'below', 'to', 'but', 'few', 'most', 'Survive', 'herself', 'itself', 'same', 'his', 'Fire', 'had', 'through', 'off', 'him', 'in', 'because', \"mustn't\", 'is', \"haven't\", 'wounded', 'survived', 'ain', 'of', 'been', 'sunk', 'while', 'from', 'each', 'he', 'between', \"doesn't\", \"you'd\", 'above', 'with', 'won', 'doing', \"won't\", 'nor', 'ablaze', 'who', 'fires', 'under', 'tragedy', \"she's\", \"you'll\", 'will', 'yourself', \"it's\", 'have', 'calamities', 'has', 'about', 'haven', \"you've\", 've', 'no', 'mightn', 'couldn', 'all', 'them', 'himself', 'again', 'until', 'my', 'ourselves', 'Sunk', 'hadn', 'which', 'both', 'should', 'the', 'at', 'can', 'don', \"wasn't\", 'not', 'more', 'me', 'aren', 'for', 'its', 'she', 'as', 'Crying', \"didn't\", 'just', 'how', 'd', 'a', 'shouldn', 'there', 'it', \"shan't\", 'having', 'you', \"weren't\", 'windstorm', 'our', \"shouldn't\", 's', 'by', 'hers', 'once', 'after', 'yourselves', 'only', 'we', 'were', 'did', 'or', 'wasn', 'Survived', 'myself', 'where', 'before', \"wouldn't\", 'that', 'these', 'll', 'was', 'into', 'survive', 'being', 'very', \"should've\", 'ours', 'up', 'when', \"you're\", 'Wounded', 'why', 'against', 'be', 'too', 'and', 'than', 't', 'hasn', 'whom', 'so', 'their', \"needn't\", 'other', 'isn', 'needn', 'those', 'this', 'your', 'what', 'didn', 'y', 'then', 'am'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vibhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "include_stopwords = {'Crying', 'ablaze', 'Ablaze', 'fall', \"calamities\", 'windstorm', 'Windstorm', \"fires\", 'Fire', 'wounded', 'Wounded', 'sunk', 'Sunk',\n",
    "                    'Survive', 'survive', 'Survived', 'survived', 'storm', 'trapped', 'tragedy'\n",
    "                    }\n",
    "exclude_stopwords = {'during'}\n",
    "\n",
    "stopwords |= include_stopwords\n",
    "stopwords -= exclude_stopwords\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.89      0.82       851\n",
      "           1       0.83      0.66      0.73       672\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.80      0.77      0.78      1523\n",
      "weighted avg       0.79      0.79      0.78      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build a text processing and classifier pipeline\n",
    "# to predict the country (USA or Canada) of a speech\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df2 = df\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df2['text'], df2['target'], test_size=0.2)\n",
    "\n",
    "# Create a pipeline that first transforms the text data into TF-IDF vectors, then applies SVM\n",
    "text_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=list(stopwords))),\n",
    "    ('clf', svm.SVC()),\n",
    "])\n",
    "\n",
    "# Train the classifier\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script creates a new column 'sentiment' in the dataframe, \n",
    "# which contains the sentiment score of the text. \n",
    "# The sentiment score is a float within the range [-1.0, 1.0], \n",
    "# where -1.0 denotes a very negative sentiment, \n",
    "# 1.0 denotes a very positive sentiment, \n",
    "# and values around 0 denote a neutral sentiment.\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Define a function to apply sentiment analysis to a text\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity  # returns a value between -1 and 1\n",
    "\n",
    "# Create a new column 'sentiment' in the DataFrame\n",
    "df2['sentiment'] = df2['text'].apply(get_sentiment)\n",
    "\n",
    "# Display the DataFrame\n",
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    0.070622\n",
       "1    0.018631\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find average sentiment for each country in df2\n",
    "df2.groupby('target')['sentiment'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keyword\n",
       "hazardous    0.457891\n",
       "razed        0.418946\n",
       "outbreak     0.312661\n",
       "mayhem       0.277262\n",
       "wreckage     0.273440\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find average sentiment for each speaker in df2\n",
    "# order the results from most positive to most negative\n",
    "\n",
    "df2.groupby('keyword')['sentiment'].mean().sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location\n",
       "The Waystone Inn               1.0\n",
       "The Main                       1.0\n",
       "Morocco                        1.0\n",
       "Paranaque City                 1.0\n",
       "Mostly Yuin.                   1.0\n",
       "                              ... \n",
       "fujo garbage heaven           -1.0\n",
       "Freeport IL. USA              -1.0\n",
       "Mumbai india                  -1.0\n",
       "sri lanka                     -1.0\n",
       "Deployed in the Middle East   -1.0\n",
       "Name: sentiment, Length: 3341, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.groupby('location')['sentiment'].mean().sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
